# Building a Docker image for LlamaEdge with Meta Llama 3.2 3B Instruct model

# Stage 1: Builder
# This stage downloads Wasm application, LLM model, and installs WasmEdge.
FROM debian:bookworm-slim AS builder

# Install dependencies required for downloading artifacts and installing WasmEdge.
RUN apt-get update && \
    apt-get install -y curl git python3 ca-certificates && \
    rm -rf /var/lib/apt/lists/*

# Set working directory for downloads and WasmEdge installation.
WORKDIR /opt/build

# Download Wasm application and LLM model.
RUN curl -LO https://github.com/second-state/LlamaEdge/releases/latest/download/llama-api-server.wasm

# Download the Meta Llama 3.2 3B Instruct model in GGUF format.
RUN curl -LO https://huggingface.co/second-state/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q5_K_M.gguf

# Install WasmEdge to /opt/wasmedge_install in the builder stage.
# The -p flag directs the installation to a specific prefix.
RUN curl -sSf https://raw.githubusercontent.com/WasmEdge/WasmEdge/master/utils/install_v2.sh | bash -s -- -p /opt/wasmedge_install


# Stage 2: Runtime
# This stage sets up the runtime environment by copying WasmEdge,
# the application, and the model from the builder stage.
FROM debian:bookworm-slim AS runtime

# Set the working directory for the application.
WORKDIR /

# Copy the WasmEdge installation (binaries, libs, plugins) from the builder stage.
COPY --from=builder /opt/wasmedge_install /opt/wasmedge_install

# Copy the Wasm binary from the builder stage.
COPY --from=builder /opt/build/llama-api-server.wasm /llama-api-server.wasm
# Copy the GGUF model from the builder stage.
COPY --from=builder /opt/build/Llama-3.2-3B-Instruct-Q5_K_M.gguf /Llama-3.2-3B-Instruct-Q5_K_M.gguf

# Add WasmEdge binaries to the PATH environment variable.
ENV PATH="/opt/wasmedge_install/bin:${PATH}"

# Define the command to run the LlamaEdge API server with WasmEdge.
CMD ["wasmedge", \
    "--dir", ".:.", \
    "--nn-preload", "default:GGML:AUTO:Llama-3.2-3B-Instruct-Q5_K_M.gguf", \
    "llama-api-server.wasm", \
    "--prompt-template", "llama-3-tool", \
    "--ctx-size", "128000", \
    "--model-name", "Llama-3.2-3b"]